---
title: 梯度求解学习小结3
date: 2017-12-22 12:38:00
categories:
  - 机器学习
tags: 
  - loss
  - numpy
---

# 问题
在{% post_link 梯度求解学习小结1 梯度求解学习小结1 %}和{% post_link 梯度求解学习小结2 梯度求解学习小结2 %}中，分别学习了如何求解损失对打分的梯度和全连接层梯度的求解。但是现在的神经网络里，除了全连接层，普遍还会加入conv、rnn/lstm、bn等层，本小节仅总结这三种结构的梯度求解过程，设计前向和后项两个阶段，不涉及上述结构诸如“优缺点”、“作用”等较为上层的总结。

conv、rnn/lstm、bn这几种结构比全连接层稍稍复杂，不容易用数学的形式化语言描述，因此本小节将重点放在代码部分，借住numpy尽量简化代码，帮助我在脑海中形成一个尽量简洁的过程。

# 卷积层
建议先阅读[convolutional-networks](http://cs231n.github.io/convolutional-networks/)理解卷积层的前向传播过程。这个过程大致如下：


```python
def conv_forward_naive(x, w, b, conv_param):
  """
  A naive implementation of the forward pass for a convolutional layer.

  The input consists of N data points, each with C channels, height H and
  width W. We convolve each input with F different filters, where each filter
  spans all C channels and has height HH and width HH.

  Input:
  - x: Input data of shape (N, C, H, W)
  - w: Filter weights of shape (F, C, HH, WW)
  - b: Biases, of shape (F,)
  - conv_param: A dictionary with the following keys:
    - 'stride': The number of pixels between adjacent receptive fields in the
      horizontal and vertical directions.
    - 'pad': The number of pixels that will be used to zero-pad the input.

  Returns a tuple of:
  - out: Output data, of shape (N, F, H', W') where H' and W' are given by
    H' = 1 + (H + 2 * pad - HH) / stride
    W' = 1 + (W + 2 * pad - WW) / stride
  - cache: (x, w, b, conv_param)
  """
  (N, C, H, W) = x.shape
  (F, C, HH, WW) = w.shape
  D = C * HH * WW
  w1 = w.reshape((F, D)).T
  pad, stride = conv_param['pad'], conv_param['stride']

  x = np.pad(x, ((0, 0), (0, 0), (pad, pad), (pad, pad)), mode='constant')
  H1 =  1 + (H + 2 * pad - HH) // stride
  W1 = 1 + (W + 2 * pad - WW) // stride
  out = np.zeros((N, F, H1, W1))
  
  for i in range(H1):
    h_st, h_ed = i * stride, i * stride + HH
    for j in range(W1):
      w_st, w_ed = j * stride, j * stride + WW
      out[..., i, j] = x[..., h_st: h_ed, w_st: w_ed].reshape((N, D)).dot(w1) + b
  cache = (x, w, b, conv_param)
  return out, cache


def conv_backward_naive(dout, cache):
  """
  A naive implementation of the backward pass for a convolutional layer.

  Inputs:
  - dout: Upstream derivatives.
  - cache: A tuple of (x, w, b, conv_param) as in conv_forward_naive

  Returns a tuple of:
  - dx: Gradient with respect to x
  - dw: Gradient with respect to w
  - db: Gradient with respect to b
  """
  (x, w, b, conv_param) = cache
  pad, stride = conv_param['pad'], conv_param['stride']
  
  (N, F, H1, W1) = dout.shape
  (F, C, HH, WW) = w.shape
  D = C * HH * WW
  w1 = w.reshape((F, D)).T
  
  dx = np.zeros_like(x)
  dw1, db = np.zeros_like(w1), np.zeros_like(b)
  block_shape = (N, C, HH, WW)
  
  for i in range(H1):
    h_st, h_ed = i * stride, i * stride + HH
    for j in range(W1):
      w_st, w_ed = j * stride, j * stride + WW
      dx[..., h_st: h_ed, w_st: w_ed] += dout[..., i, j].dot(w1.T).reshape(block_shape)
      dw1 += x[..., h_st: h_ed, w_st: w_ed].reshape((N, -1)).T.dot(dout[..., i, j])
      db += dout[..., i, j].sum(axis=0)
  
  dx = dx[..., pad: -pad, pad: -pad]
  dw = dw1.T.reshape(w.shape)
  return dx, dw, db
```