---
title: 梯度求解学习小结1
tags: 
  - loss
  - numpy
---

# 问题
在机器学习中，设计合理、有效的目标函数是为人所津津乐道的技能（本小学生尚无此功力）。倘若设计出来的目标函数连自己都不会求解(优化)那就很尴尬了。像我这种小学生瞎鼓捣出来的目标函数,想知道究竟能不能work,不求解一下写成代码在数据集上跑一跑又如何知晓？

纵观求解方法，有贪心的，动态规划的，蒙特卡洛的，期望最大的，梯度的等等（小学生无责任乱分）。前途无限的深度学习其求解方法全都依赖于梯度，在可以预计的将来极有可能成为大一统的求解方法。因此，如何求解损失函数（这里的目标函数可以称作损失函数）的梯度成了小学生心中最关键的一环。求解梯度大体可以分为3个步骤：
1. 写出梯度解析解的表达式；
2. 代码实现解析解；
3. 用数值解验证解析解。

# 梯度的解析解
如何求解梯度建议先看看cs231n lecture 4 的[ppt](http://10.254.1.82/cache/7/03/cs231n.stanford.edu/fa3f0bb005b28102ff19490aea79b536/cs231n_2017_lecture4.pdf)或者[vidio](https://www.youtube.com/watch?v=d14TUNcbn1k&list=PL3FW7Lu3i5JvHM8ljYj-zLfQRF3EO8sYv)(cs231n的其他学习资料在[这里](http://cs231n.stanford.edu/syllabus.html))。

我们所谓的损失指的都是期望损失，也就是平均损失。一些表达上的技巧可以帮助我们后面更好地计算梯度.

## 使用逻辑判断与内积代替选择
### 二值选择
像$L=\max(x^2, e^y)$这样的二元选择表达式，它的偏导为

$$\frac{\partial L}{\partial x}=\Big\{ \begin{matrix}
  2x &  x^2 \geq e^y \\
  0 &  x^2 < e^y
\end{matrix}$$

$$\frac{\partial L}{\partial y}=\Big\{ \begin{matrix}
  0 &  x^2 \geq e^y \\
  e^y &  x^2 < e^y
\end{matrix}$$

**注意**:相等的情况视需求而定。

用逻辑判断可以表示为

$$L=1\{x^2 \geq e^y\} \cdot x^2 + 1\{x^2 < e^y\}e^y$$
那么，逻辑判断因子可以看做一个常数，它的偏导就可以表示为

$$\frac{\partial L}{\partial x}=1\{x^2 \geq e^y\} \cdot 2x$$
$$\frac{\partial L}{\partial y}=1\{x^2 < e^y\} \cdot e^y$$

### 多值选择：
假设S是一个长度为3的数组，像$L=9S_2$这样的表达式，它的偏导为

$$\frac{\partial L}{\partial S_1}=0$$
$$\frac{\partial L}{\partial S_2}=9$$
$$\frac{\partial L}{\partial S_3}=0$$

用逻辑判断与内积可以表示为

$$L=\sum_i1\{i=2\}\cdot 9S_i$$
其中i=1,2,3。那么它的偏导就可以表示为

$$\frac{\partial L}{\partial S_i}=\sum_i1\{i=2\}\cdot 9, i=1,2,3$$

## 梯度解析式计算
### 对矩阵或向量求导
众所周知，求解的目标是$\frac{\partial L}{\partial W}$, 对于一条样本而言，$loss_i=L(f(x_i;W), y_i)$的W是一个向量或者矩阵，如在神经网络中，每一层的W都以矩阵的形式存在。很显然W的形状由输入和输出的维度决定：

$$X_{(1 \times D)} \cdot W_{(D \times K)} = Y_{(1 \times K)}$$
$$X_{(2 \times D)} \cdot W_{(D \times K)} = Y_{(2 \times K)}$$
$$……$$
$$X_{(N \times D)} \cdot W_{(D \times K)} = Y_{(N \times K)}$$

当模型f输出为实数时，如二分类和回归的情况，W为向量：

$$X_{(1 \times D)} \cdot W_{(D \times 1)} = Y_{(1 \times 1)}$$

因为损失函数的输出是实数，在{% post_link 4矩阵分析 矩阵分析 %}中，学习过，实数函数对向量或矩阵求导就是实数函数对向量或矩阵中的每个分量求偏导，若W是一个列行量则：

$$\frac{\partial L}{\partial W}=\bigg(\frac{\partial L}{\partial W_1}, \frac{\partial L}{\partial W_1}, ..., \frac{\partial L}{\partial W_D}\bigg)$$

若W是一个矩阵则：

$$\frac{\partial L}{\partial W}=\left(\begin{matrix}
\frac{\partial L}{\partial W_{11}} & \frac{\partial L}{\partial W_{12}} & ... &\frac{\partial L}{\partial W_{1K}} \\
\frac{\partial L}{\partial W_{21}} & \frac{\partial L}{\partial W_{22}} & ... &\frac{\partial L}{\partial W_{2K}} \\
\vdots & \vdots &  & \vdots \\
\frac{\partial L}{\partial W_{D1}} & \frac{\partial L}{\partial W_{D2}} & ... &\frac{\partial L}{\partial W_{DK}} 
\end{matrix}\right)$$

也可以表示成行向量的形式：

$$\frac{\partial L}{\partial W}=\bigg(\frac{\partial L}{\partial W_{\cdot 1}}, \frac{\partial L}{\partial W_{\cdot 2}}, ..., \frac{\partial L}{\partial W_{\cdot K}}\bigg)$$
其中$\frac{\partial L}{\partial W_{\cdot j}}$表示矩阵表达式中的第j列:

$$\frac{\partial L}{\partial W_{\cdot j}}=\bigg(\frac{\partial L}{\partial W_{1j}}, \frac{\partial L}{\partial W_{2j}}, ..., \frac{\partial L}{\partial W_{Dj}}\bigg)^T$$

### 链式法则
$Loss_i=L(f(x_i; W), y_i)$中若$L(.)$非常复杂，则需要借助链式法则求解

$$\frac{\partial L}{\partial W}=\frac{\partial L}{\partial f}\cdot\frac{\partial f}{\partial W}$$
若模型f的输出有多个维度，如K分类，则

$$\frac{\partial L}{\partial W}=\sum_{j=1}^K\frac{\partial L}{\partial f_j}\cdot\frac{\partial f_j}{\partial W}$$

若模型f为多层神经网络，$\partial f_j/\partial W$的求解也需使用链式法则，此处不展开。


## 实际中的例子
### 1.Multiclass SVM loss
$$L=\sum_{j\neq y}\max(0, s_j-s_y+1)$$
其中s为某样本属于各类别的打分，若是k分类，s为长度为K的数组，为了表达它是一个行向量，以下标记为$s^T$, $s_j$为该样本属于第j类的打分，y为真实类别，详情请参见该[ppt](http://10.254.1.82/cache/17/03/cs231n.stanford.edu/04621c4e4ccd36036cba1c24720d099d/cs231n_2017_lecture3.pdf)。
打分的计算模型为

$$s^T=f(x; W) = x^T \cdot W$$
其中，x为一条1xD的输入,W为DxK的矩阵

根据链式法则，有

$$\frac{\partial L}{\partial W}=\sum_{m=1}^K\frac{\partial L}{\partial s_m}\cdot\frac{\partial s_m}{\partial W}$$

把L中的二值选择用逻辑判断代替

$$L=\sum_j^K1\{j \neq y\}1\{s_j-s_y+1 \geq 0\}(s_j-s_y+1)$$
则上面链式法则等式中的第一部分偏导为

$$\begin{aligned}
\frac{\partial L}{\partial s_m} &= \sum_j^K1\{j \neq y\}1\{s_j-s_y+1 \geq 0\}\frac{\partial (s_j-s_y+1)}{\partial s_m} \\
&= \sum_j^K1\{j \neq y\}1\{s_j-s_y+1 \geq 0\}\bigg(1\{m \neq y\} - 1\{m=y\}\bigg)
\end{aligned}$$

L为实数函数，$s_m$为实数，因此$\partial L/\partial s_m$也为实数。<br>
对于另一部分偏导，因为

$$s_j=x^T \cdot W_{\cdot j}$$
$$\frac{\partial s_j}{\partial W_{\cdot j}}=x$$
所以上面链式法则等式中的第二部分偏导为：

$$\begin{aligned}\frac{\partial s_m}{\partial W} &=\bigg(\frac{\partial s_m}{\partial W_{\cdot 1}}, ..., \frac{\partial s_m}{\partial W_{\cdot m}}, ..., \frac{\partial s_m}{\partial W_{\cdot K}}\bigg) \\
& = \bigg(0, ..., \frac{\partial s_m}{\partial W_{\cdot m}}, ..., 0\bigg)
\end{aligned}$$
因此，

$$\begin{aligned}\frac{\partial L}{\partial W}
&= \sum_{m=1}^K\frac{\partial L}{\partial s_m}\cdot\frac{\partial s_m}{\partial W} \\
&= \sum_{m=1}^K\frac{\partial L}{\partial s_m} \cdot \bigg(0, ..., \frac{\partial s_m}{\partial W_{\cdot m}}, ..., 0\bigg)\\
&= \sum_{m=1}^K\frac{\partial L}{\partial s_m} \cdot \bigg(0, ..., x, ..., 0\bigg)\\
&= \bigg(\frac{\partial L}{\partial s_1} \cdot x, ...,\frac{\partial L}{\partial s_K} \cdot x\bigg) \\
&=x \cdot\bigg(\frac{\partial L}{\partial s_1}, ...,\frac{\partial L}{\partial s_K}\bigg)\\
\end{aligned}$$

>**注意**
>
>$$\left(\begin{matrix} 1 \cdot \left(\begin{matrix} 3 \\ 4 \end{matrix}\right), & 2\cdot \left(\begin{matrix} 3 \\ 4 \end{matrix}\right) \end{matrix}\right) = \left(\begin{matrix} 3 \\ 4 \end{matrix}\right) \cdot (1 ,2)$$
>
将$\partial L /\partial W$写成分量的形式（按列分）

$$\begin{aligned}\bigg(\frac{\partial L}{\partial W}\bigg)_{\cdot m} 
&=\frac{\partial L}{\partial s_m} \cdot x \\
&=\bigg(\sum_j^K1\{j \neq y\}1\{s_j-s_y+1 \geq 0\}\Big(1\{m \neq y\} - 1\{m=y\}\Big)\bigg) \frac{\partial s_m}{\partial W_{\cdot m}} \\
\end{aligned}$$
其中$\partial s_m/\partial W_{\cdot m}$是实数对列向量求导，所以结果也是列向量，即行向量$\partial s_m/\partial W$中的每个分量是列向量，那么$\partial s_m/\partial W$是一个和矩阵，形状与W相同。



### 2.Softmax loss
$$L=-\log\bigg(\frac{exp(s_y)}{\sum_jexp(s_j)}\bigg)$$
可以转化为

$$L=-\sum_k1\{k=y\}\log\bigg(\frac{exp(s_k)}{\sum_jexp(s_j)}\bigg)$$

## 
# 总结

