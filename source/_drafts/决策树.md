---
title: 决策树
tags:
---

从数学上看，决策树是一个分段函数，它把特征空间划分成许多子空间，用子空间局部的样本预测被分到这个子空间的新样本，以众数分类，以均值回归，它抓住了特征空间的局部性特性。

![](http://i2.nbimg.com/602416/5e4c740208dd55cb.png)

$$f(x;\{R_j,b_j\}^J_1)=\sum^J_{j=1}b_j1(x \in R_j)$$

决策树的关键就在于如何分割。用一种简单的方法，每次从特征空间中选出一个维度进行分割（后面再说怎么选）。如果这个维度的特征是**离散型**的那就比较好分，有几个值就分割成几段。如果这个维度的特征是**连续型**的就麻烦一点。过程大致是这样：先把该特征里的所有连续值的去重后的值排序，在值与值之间设置候选分割点，从这些分割点中选一个把特征分为2段，与离散型特征不同的是，连续型可能还会被选出来再分一次。

接下来的问题是怎么选出**分割的维度**和**连续型特征的分割点**。决策树的理想状态是每个子空间内的样本尽量一致（分类：类别尽量一直；回归：方差尽量小）。对于分类，这里衍生出了三种选择准则。

### (1)信息增益
$$g(D,A)=H(D)-H(D|A)$$

H(D)是训练集D中样本类别的信息熵，比如有K个类别，1,2,...,K，H(D)就是由这k种编码每种编码在训练集中出现的概率$p_k$求出的信息熵，若在某个子空间中的样本为D，样本量为|D|,第k类的样本有$|D_k|$个，那么

$$H(D)=-\sum_{k=1}^Kp_k\log p_k$$
$$p_k=\frac{|D_k|}{|D|}$$

H(D|A)是给定条件A下的D的条件熵,D在给定特征A不同取值后的信息熵的期望。比如想用特征A分割，特征A有三个取值：a1,a2,a3，当前待分割的子空间内的样本量为|D|这3个取值的样本量有$|D_{a1}|$

$$H(D|A)=-\sum_{a \in A}p(a)H(D|A=a)$$


