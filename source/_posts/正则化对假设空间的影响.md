---
title: 正则化对假设空间的影响
date: 2017-10-13 13:03:23
categories:
  - 机器学习
tags: 
  - 正则化
  - 贝叶斯
---

正则项的引入可以降低模型的复杂度，增强模型的泛化能力，对于这一点我的理解比较模糊，直到学习到了正则项的贝叶斯解释才逐渐清晰。
最终的推理逻辑是：<br>
1. 给优化目标加入正则项等价于给模型参数引入先验；
2. 这种先验限制模型参数的置信空间，达到了限制模型假设空间的作用；
3. 模型假设空间小了，表达能力削弱后不容易学习到噪声，因此不容易出现过拟合，泛化能力得到增强。

# 正则化的贝叶斯解释
## 约定
训练集：{% raw %}$\{x_i, y_i\}^N_1${% endraw %}；模型：{% raw %}$y_i=f(x_i; w)${% endraw %}。<br>
其中N为样本量，{% raw %}$x_i${% endraw %}为模型输入，{% raw %}$y_i${% endraw %}为模型输出{% raw %}$(1 \leq i \leq N)${% endraw %}, {% raw %}$w${% endraw %}为模型参数,这里{% raw %}$x_i${% endraw %}和{% raw %}$w${% endraw %}为{% raw %}$K${% endraw %}维，{% raw %}$w=[w_1,...,w_j,...,w_K]^T${% endraw %},{% raw %}$(1 \leq j \leq K)${% endraw %}

贝叶斯观点看来，模型参数{% raw %}$w${% endraw %}是一个K维的随机变量，它存在一个先验概率{% raw %}$P(w)${% endraw %},并可以通过最大化后验概率{% raw %}$\max_w P(w|x,y)${% endraw %}来求解最能拟合训练集{% raw %}$\{x_i, y_i\}^N_1${% endraw %}的{% raw %}$w${% endraw %}。在最大化后验概率的过程中会用到{% raw %}$w${% endraw %}的先验概率{% raw %}$P(w)${% endraw %}和似然{% raw %}$P(y|x, w)${% endraw %}（这里没写成{% raw %}$P(y|x; w)${% endraw %}是把{% raw %}$w${% endraw %}当做随机变量来看待）。<br>

我们假设w的各个维度相互独立，且先验概率服从均值为0，标准差为a的高斯分布

{% raw %}$$w_j \sim N(0,a^2)$${% endraw %}
有

{% raw %}$$P(w)=\prod_j^KP(w_j)= \prod_j^K\frac{1}{a \sqrt{2 \pi}}exp(-\frac{w_j^2}{2a^2})$${% endraw %}

同时假设{% raw %}$y_i${% endraw %}是独立的，{% raw %}$y_i${% endraw %}的噪声服从高斯分布，即{% raw %}$y_i${% endraw %}服从均值为{% raw %}$f(x_i, w)${% endraw %}，标准差为b的高斯分布（独立同分布）

{% raw %}$$y_j \sim N(f(x_i, w),b^2)$${% endraw %}
有

{% raw %}$$P(y|x, w)=\prod_i^NP(y_i|x_i, w) = \prod_i^N\frac{1}{b \sqrt{2 \pi}}exp(-\frac{(y-f(x_i, w))^2}{2b^2})$${% endraw %}

## 最大化后验估计

{% raw %}$$\begin{aligned}
\max_w P(w|x,y) &= \max \frac{P(y|x,w)P(w)}{\sum_wP(y|x,w)P(w)} = \max P(y|x,w)P(w) \\
&= \max \prod_i^N\frac{1}{b \sqrt{2 \pi}}exp(-\frac{(y-f(x_i, w))^2}{2b^2}) \cdot \prod_j^K\frac{1}{a \sqrt{2 \pi}}exp(-\frac{w_j^2}{2a^2}) \\
&= \max \sum_i^N \ln \bigg( \frac{1}{b \sqrt{2 \pi}}exp(-\frac{(y-f(x_i, w))^2}{2b^2})\bigg) + \sum_j^K \ln \bigg( \frac{1}{a \sqrt{2 \pi}}exp(-\frac{w_j^2}{2a^2}) \bigg) \\
&= \max \sum_i^N \Bigg\{ \ln \bigg( \frac{1}{b \sqrt{2 \pi}}\bigg)-\frac{(y-f(x_i, w))^2}{2b^2} \Bigg\} + \sum_j^K \Bigg\{ \ln \bigg( \frac{1}{a \sqrt{2 \pi}} \bigg) -\frac{w_j^2}{2a^2} \Bigg\} \\
&= \max \sum_i^N \Bigg\{ -\frac{(y-f(x_i, w))^2}{2b^2} \Bigg\} + \sum_j^K \Bigg\{  -\frac{w_j^2}{2a^2} \Bigg\} \\
&= \max -\frac{1}{2b^2}\sum_i^N (y-f(x_i, w))^2 - \frac{1}{2a^2}\sum_j^K w_j^2 \\ 
&= \max - \sum_i^N (y-f(x_i, w))^2 - \frac{b^2}{a^2} \sum_j^K w_j^2 \\
&= \min \sum_i^N (y-f(x_i, w))^2 + \lambda \sum_j^K w_j^2
\end{aligned}$${% endraw %}

其中第一行用到了贝叶斯公式，且贝叶斯公式的分母是定值，在最大化中被舍去；第二行引入了约定中的两个独立高斯假设；第五行舍去了两个常量{% raw %}$\ln \big( \frac{1}{b \sqrt{2 \pi}}\big)${% endraw %}和{% raw %}$\ln \big( \frac{1}{a \sqrt{2 \pi}}\big)${% endraw %}；倒数第三行给目标函数乘上常量{% raw %}$2b^2${% endraw %}；最后一行去掉负号改为最小化，并令

{% raw %}$$\lambda = \frac{b^2}{a^2}$${% endraw %}

最终，可见**高斯噪声假设和高斯先验假设的最大化后验概率，等价于带L2正则项的最小化平方损失**。

## 补充：拉普拉斯先验
L1正则也是常用的正则化形式，为此我们将上面约定中对参数先验的假设改为<a href="https://zh.wikipedia.org/wiki/%E6%8B%89%E6%99%AE%E6%8B%89%E6%96%AF%E5%88%86%E5%B8%83" title="拉布拉斯分布">拉布拉斯分布</a>，如

{% raw %}$$w_j \sim L(0,a), (a>0)$${% endraw %}
有

{% raw %}$$P(w) = \prod_j^K\frac{1}{2a}exp(-\frac{|w_j|}{a}), (a>0)$${% endraw %}

则最大化后验概率的过程为

{% raw %}$$\begin{aligned}
\max_w P(w|x,y) &= \max \frac{P(y|x,w)P(w)}{\sum_wP(y|x,w)P(w)} = \max P(y|x,w)P(w) \\
&= \max \prod_i^N\frac{1}{b \sqrt{2 \pi}}exp(-\frac{(y-f(x_i, w))^2}{2b^2}) \cdot \prod_j^K\frac{1}{2a}exp(-\frac{|w_j|}{a}) \\
&= \max \sum_i^N \Bigg\{ \ln \bigg( \frac{1}{b \sqrt{2 \pi}}\bigg)-\frac{(y-f(x_i, w))^2}{2b^2} \Bigg\} + \sum_j^K \Bigg\{ \ln \bigg( \frac{1}{2a} \bigg) -\frac{|w_j|}{a} \Bigg\} \\
&= \max -\frac{1}{2b^2}\sum_i^N (y-f(x_i, w))^2 - \frac{1}{a}\sum_j^K |w_j| \\ 
&= \max - \sum_i^N (y-f(x_i, w))^2 - \frac{2b^2}{a} \sum_j^K |w_j| \\
&= \min \sum_i^N (y-f(x_i, w))^2 + \lambda \sum_j^K |w_j|
\end{aligned}$${% endraw %}

这里最后一行，令

{% raw %}$$\lambda = \frac{2b^2}{a}, (a>0)$${% endraw %}

# 模型复杂度
模型复杂度可以理解为模型假设空间的大小。高维模型，多节点的树模型，这些模型可表达的函数更多，假设空间更大，通俗地说法就是模型复杂度高。

把正则项引入优化目标函数里来，是为了限制在优化损失时模型的表达能力，不要把过于细节的东西学习进来，减少过拟合。
看l2这个例子，我们不能说{% raw %}$y=3x${% endraw %}比{% raw %}$y=2x${% endraw %}更复杂，也不能说{% raw %}$y=100x${% endraw %}比{% raw %}$y=2x^2+2x${% endraw %}复杂；但是我们能说 {% raw %}$y=ax^2+bx${% endraw %}比{% raw %}$y=bx${% endraw %}复杂。还能说{% raw %}$y=wx,w^2<20${% endraw %}比{% raw %}$y=wx,w^2<2${% endraw %}复杂，因为{% raw %}$w^2<20${% endraw %}的假设空间比{% raw %}$w^2<2${% endraw %}更大。

在正则化中，L2正则即假设模型参数服从高斯分布，如带正态先验的一维线性模型

{% raw %}$$y=wx, w \sim N(0,20^2)$${% endraw %}

这个模型的参数w的95%置信区间是(-39.2, 39.2)，有95%的概率，w的取值被限制在(-39.2, 39.2)，如此一来，模型的假设空间就被限制住，学习能力就低了。

对于另一个例子，{% raw %}$y=wx, w \sim N(0,2^2)${% endraw %}，虽然这两个模型的参数空间都是R, 但是后者模型的参数大概率出现在范围更小的空间内, 其参数w的95%置信区间是(-3.92, 3.92)，在同样的置信水平下，{% raw %}$y=wx, w \sim N(0,2^2)${% endraw %}的假设空间更小，复杂度更小，学习能力更低，更加不容易学习数据中的细节，如噪声。

为什么要使学习能力变低呢？简单地说，增强学习能力，降低学习能力，（增强模型复杂度，降低模型复杂度）这只是一种调节手段，当我们模型维度低时，我们只能学习数据中50%的知识。增加模型的维度后我们能学习数据中95%的知识，可是数据中只有80%的知识是有价值的，剩下20%是糟粕。因此我们有需要适当降一降模型复杂度，降一降学习能力，使我们能尽量只学习数据中有用的80%。

# 总结
1. 给损失函数加入正则项进行优化等价于给模型参数引入先验；
2. 这种先验限制了模型假设空间；
3. 模型假设空间小了，表达能力削弱后不容易学习到噪声，因此不容易出现过拟合，泛化能力得到增强。
