---
title: 梯度求解学习小结1
date: 2017-11-20 19:53:00
categories:
  - 机器学习
tags: 
  - loss
  - numpy
---

# 问题
在机器学习中，设计合理、有效的目标函数是为人所津津乐道的技能（本小学生尚无此功力）。倘若设计出来的目标函数连自己都不会求解(优化)那就很尴尬了。像我这种小学生瞎鼓捣出来的目标函数,想知道究竟能不能work,不求解一下写成代码在数据集上跑一跑又如何知晓？

纵观求解方法，有贪心的，动态规划的，蒙特卡洛的，期望最大的，梯度的等等（小学生无责任乱分）。前途无限的深度学习其求解方法全都依赖于梯度，在可以预计的将来极有可能成为大一统的求解方法。因此，如何求解损失函数（这里的目标函数可以称作损失函数）的梯度成了小学生心中最关键的一环。

如何求解梯度建议先看看cs231n lecture 4 的[ppt](http://10.254.1.82/cache/7/03/cs231n.stanford.edu/fa3f0bb005b28102ff19490aea79b536/cs231n_2017_lecture4.pdf)或者[vidio](https://www.youtube.com/watch?v=d14TUNcbn1k&list=PL3FW7Lu3i5JvHM8ljYj-zLfQRF3EO8sYv)(cs231n的其他学习资料在[这里](http://cs231n.stanford.edu/syllabus.html))。

一些表达上的技巧可以帮助我们后面更好地计算梯度.

# 梯度推导过程中的trick

## 逻辑判断
把一些分段函数化为便于求导的乘积形式，多少有点用吧。
### 二值选择
{% raw %}

$$\min(x^2, e^y)=1\{x^2 < e^y\}x^2 + 1\{x^2 \geq e^y\}e^y$$
$$\max(0, x^3)=1\{x^3 \geq 0\}x^3$$
{% endraw %}

### 多值选择
{% raw %}

$$Y_7=\sum_i1\{i=7\} Y_i$$
$$\sum_x 1\{x=2\}f(x)=f(2)$$
{% endraw %}

### 分组讨论：
{% raw %}

$$\begin{aligned}\frac{\partial(2s_1-3s_2+e^{s_3})}{\partial s_i} &= \Bigg\{\begin{matrix} 2, & i=1 \\ -3, & i=2  \\ e^{s_3}, & i=3\end{matrix} \\
&= 1\{i=1\}2 - 1\{i=2\}3 + 1\{i=3\}e^{s_3}
\end{aligned}$$
{% endraw %}
### 逻辑运算
1. 与运算<br>
{% raw %}

$$1\{x > 5 \land x < 10\} = 1\{x > 5\} \cdot 1\{x < 10\}$$
{% endraw %}

2. 或运算<br>{% raw %}

$$1\{x > 5 \lor x < 10\} = 1\{x > 5\} + 1\{x < 10\}$${% endraw %}


## 链式法则
{% raw %}$Loss_i=L(f(x_i; W), y_i)${% endraw %}中若$L(.)$非常复杂，则需要借助链式法则求解
{% raw %}

$$\frac{\partial L}{\partial W}=\frac{\partial L}{\partial f}\cdot\frac{\partial f}{\partial W}$${% endraw %}
若模型f的输出有多个维度，如K分类，则
{% raw %}

$$\frac{\partial L}{\partial W}=\sum_{j=1}^K\frac{\partial L}{\partial f_j}\cdot\frac{\partial f_j}{\partial W}$${% endraw %}

若模型f为多层神经网络，{% raw %}$\partial f_j/\partial W${% endraw %}的求解也需使用链式法则，此处不展开。

## 矩阵求导
众所周知，求解的目标是$\frac{\partial L}{\partial W}$, 对于一条样本而言，{% raw %}$loss_i=L(f(x_i;W), y_i)${% endraw %}的W是一个向量(2分类)或者矩阵(多分类)，如在神经网络中，每一层的W都以矩阵的形式存在。很显然W的形状由输入和输出的维度决定：
{% raw %}

$$X_{(1 \times D)} \cdot W_{(D \times K)} = Y_{(1 \times K)}$$
$$X_{(2 \times D)} \cdot W_{(D \times K)} = Y_{(2 \times K)}$$
$$……$$
$$X_{(N \times D)} \cdot W_{(D \times K)} = Y_{(N \times K)}$${% endraw %}

因为损失函数的输出是实数，在{% post_link 4矩阵分析 矩阵分析 %}中，学习过，实数函数对向量或矩阵求导就是实数函数对向量或矩阵中的每个分量求偏导，若W是一个$D \times 1$的行向量则：
{% raw %}

$$\frac{\partial L}{\partial W}=\bigg(\frac{\partial L}{\partial W_1}, \frac{\partial L}{\partial W_2}, ..., \frac{\partial L}{\partial W_D}\bigg)$${% endraw %}

若W是一个$D \times K$的矩阵则：
{% raw %}

$$\frac{\partial L}{\partial W}=\left(\begin{matrix}
\frac{\partial L}{\partial W_{11}} & \frac{\partial L}{\partial W_{12}} & ... &\frac{\partial L}{\partial W_{1K}} \\
\frac{\partial L}{\partial W_{21}} & \frac{\partial L}{\partial W_{22}} & ... &\frac{\partial L}{\partial W_{2K}} \\
\vdots & \vdots &  & \vdots \\
\frac{\partial L}{\partial W_{D1}} & \frac{\partial L}{\partial W_{D2}} & ... &\frac{\partial L}{\partial W_{DK}} 
\end{matrix}\right)$${% endraw %}

也可以表示成行向量的形式：
{% raw %}

$$\frac{\partial L}{\partial W}=\bigg(\frac{\partial L}{\partial W_{\cdot 1}}, \frac{\partial L}{\partial W_{\cdot 2}}, ..., \frac{\partial L}{\partial W_{\cdot K}}\bigg)$${% endraw %}
其中{% raw %}$\frac{\partial L}{\partial W_{\cdot j}}${% endraw %}表示矩阵表达式中的第j列:
{% raw %}

$$\frac{\partial L}{\partial W_{\cdot j}}=\bigg(\frac{\partial L}{\partial W_{1j}}, \frac{\partial L}{\partial W_{2j}}, ..., \frac{\partial L}{\partial W_{Dj}}\bigg)^T$${% endraw %}

# 实际中的例子

## <div id="mcsvml">Multiclass SVM loss</div>
{% raw %}

$$L=\sum_{k\neq y}^K\max(0, s_k-s_y+1)$${% endraw %}
其中s为某样本属于各类别的打分，若是k分类，s为长度为K的数组，为了表达它是一个行向量，以下标记为$s^T$, {% raw %}$s_k${% endraw %}为该样本属于第j类的打分，y为真实类别，详情请参见该[ppt](http://10.254.1.82/cache/17/03/cs231n.stanford.edu/04621c4e4ccd36036cba1c24720d099d/cs231n_2017_lecture3.pdf)。
打分的计算模型为

$$s^T=f(x; W) = x^T \cdot W$$
其中，x为一条1xD的输入,W为DxK的矩阵

### 使用链式法则
根据链式法则，有
{% raw %}

$$\frac{\partial L}{\partial W}=\sum_{j=1}^K\frac{\partial L}{\partial s_j}\cdot\frac{\partial s_j}{\partial W}$${% endraw %}

L为实数函数，{% raw %}$s_j${% endraw %}为实数，因此{% raw %}$\partial L/\partial s_j${% endraw %}也为实数, 而{% raw %}$\partial s_j/\partial W${% endraw %}为与W形状相同的矩阵。

### 求{% raw %}$\partial L/\partial s_j${% endraw %}
对于上面链式法则求和式中的第一部分偏导{% raw %}$\partial L/\partial s_j${% endraw %}，把L中的二值选择用逻辑判断代替
{% raw %}

$$\begin{aligned}L &= \sum_{m \neq y}^K \max(0, s_m-s_y+1)\\
&=\sum_{m \neq y}^K1\{s_m-s_y+1 \geq 0\}(s_j-s_y+1) \\
&=\sum_{m \neq y}^K q_{m,y} (s_m-s_y+1)
\end{aligned}$${% endraw %}
其中{% raw %}$q_{m,y}=1\{s_m-s_y+1 \geq 0\}${% endraw %}, 则上面{% raw %}$\big(\frac{\partial L}{\partial s_1}, ...,\frac{\partial L}{\partial s_K}\big)${% endraw %}中的分量为
{% raw %}

$$\begin{aligned}
\frac{\partial L}{\partial s_j} &= \frac{\partial}{\partial s_j}\sum_{m \neq y} q_{m,y}(s_m-s_y+1)\\
&= \sum_{m \neq y} q_{m,y}\frac{\partial (s_m-s_y+1)}{\partial s_j} \\
&= \sum_{m \neq y} q_{m,y}\Big(1\{j \neq y \land m=j\}- 1\{j=y\}\Big) \\
&= 1\{j \neq y\}\sum_{m \neq y} q_{m,y}1\{m=j\}- 1\{j=y\}\sum_{m \neq y} q_{m,y} \\
&= 1\{j \neq y\}q_{j,y}- 1\{j=y\}\sum_{m \neq y} q_{m,y}
\end{aligned}$${% endraw %}

> **注意：** 上面逆用了多值选择的情况
>
> {% raw %}$$\sum_m q_{m,y}1\{m=j\}=q_{j,y}$${% endraw %}
> 因为有$j \neq y$的保证，为$\sum$加上$m \neq y$的限制不会有任何影响

### 求{% raw %}$\partial s_j/\partial W${% endraw %}
对于上面链式法则求和式中的第二部分偏导{% raw %}$\partial s_j/\partial W${% endraw %}，因为
{% raw %}

$$s_j=x^T \cdot W_{\cdot j}$$
$$\frac{\partial s_j}{\partial W_{\cdot j}}=x$${% endraw %}
注意这里把x当做**列向量**对待，{% raw %}$W_{\cdot j}${% endraw %}表示矩阵W的第j列，所以：
{% raw %}

$$\begin{aligned}\frac{\partial s_j}{\partial W} &=\bigg(\frac{\partial s_j}{\partial W_{\cdot 1}}, ..., \frac{\partial s_j}{\partial W_{\cdot j}}, ..., \frac{\partial s_j}{\partial W_{\cdot K}}\bigg) \\
& = \bigg(0, ..., \frac{\partial s_j}{\partial W_{\cdot j}}, ..., 0\bigg)
\end{aligned}$${% endraw %}

### 求单样本损失对W的梯度
根据链式法则，有{% raw %}

$$\begin{aligned}\frac{\partial L}{\partial W}
&= \sum_{j=1}^K\frac{\partial L}{\partial s_j}\cdot\frac{\partial s_j}{\partial W} \\
&= \sum_{j=1}^K\frac{\partial L}{\partial s_j} \cdot \bigg(0, ..., \frac{\partial s_j}{\partial W_{\cdot j}}, ..., 0\bigg)\\
&= \sum_{j=1}^K\frac{\partial L}{\partial s_j} \cdot \bigg(0, ..., x, ..., 0\bigg)\\
&= \bigg(\frac{\partial L}{\partial s_1} \cdot x, ...,\frac{\partial L}{\partial s_K} \cdot x\bigg) \\
&= x \cdot \bigg(\frac{\partial L}{\partial s_1}, ...,\frac{\partial L}{\partial s_K}\bigg) \\
&=x \cdot \bigg(\frac{\partial L}{\partial s}\bigg)^T
\end{aligned}$${% endraw %}
其中($\partial L/\partial s)^T$是与$s^T$形状相同的行向量($1 \times K$)。又x本身是列向量($D \times 1$)，所以二者点积为矩阵($D \times K$)，与W形状相同。($\partial L/\partial s)^T$的分量{% raw %}$\partial L/\partial s_j${% endraw %}前面已经求过，即已经求出单样本损失对W的梯度。

>**注意**
>
>{% raw %}$$\left(\begin{matrix} 1 \cdot \left(\begin{matrix} 3 \\ 4 \end{matrix}\right), & 2\cdot \left(\begin{matrix} 3 \\ 4 \end{matrix}\right) \end{matrix}\right) = \left(\begin{matrix} 3 \\ 4 \end{matrix}\right) \cdot (1 ,2)$${% endraw %}

### 求平均损失对W的梯度
我们所谓的损失指的都是期望损失，也就是平均损失。若第i条样本的损失为$L^{(i)}$，则N条样本的平均损失为
{% raw %}

$$Loss= \frac{1}{N}\sum_{i=1}^N L^{(i)}$${% endraw %}
因此平均损失对W的梯度为
{% raw %}

$$\begin{aligned}dW
&= \frac{1}{N}\sum_{i=1}^N \frac{\partial L^{(i)}}{\partial W} \\
&= \frac{1}{N}\sum_{i=1}^N x^{(i)} \cdot \bigg(\frac{\partial L^{(i)}}{\partial s^{(i)}_1}, ...,\frac{\partial L^{(i)}}{\partial s^{(i)}_K}\bigg) \\
&= \frac{1}{N}\Big(x^{(1)}, ..., x^{(N)}\Big) \cdot \left(\begin{matrix}\frac{\partial L^{(1)}}{\partial s^{(1)}_1} & \cdots & \frac{\partial L^{(1)}}{\partial s^{(1)}_K} \\ \vdots & \ddots & \vdots \\ \frac{\partial L^{(N)}}{\partial s^{(N)}_1} & \cdots & \frac{\partial L^{(N)}}{\partial s^{(N)}_K}\end{matrix}\right)\\
&= \frac{1}{N}X^T \cdot dS
\end{aligned}$${% endraw %}
其中矩阵dS中的元素为
{% raw %}

$$\begin{aligned}(dS)_{ij} &= \frac{\partial L^{(i)}}{\partial s_j^{(i)}} \\
&= 1\{j \neq y^{(i)}\}q_{j,y^{(i)}}- 1\{j=y^{(i)}\}\sum_{m \neq y^{(i)}}^K q_{m,y^{(i)}}
\end{aligned}$${% endraw %}
而$X^T$是D行N列的矩阵，是输入矩阵X的转置。

### 编码实现

```python
def svm_loss(W, X, y, reg):
  """
  Inputs:
    - W: A numpy array of shape (D, K) containing weights.
    - X: A numpy array of shape (N, D) containing a minibatch of data.
    - y: A numpy array of shape (N,) containing training labels; 
         y[i] = k means that X[i] has label k, where 0 <= k < K.
  """
  N = X.shape[0]
  s = X.dot(W)

  # 求dS
  i = np.arange(N)
  dS = (s - s[i, y][:, np.newaxis] + 1 >= 0).astype('int')
  dS[i, y] = 0
  dS[i, y] = -dS.sum(axis=-1)

  dW = X.T.dot(dS) / N
  dW += reg * 2 * W # 正则项的梯度
```

## Softmax loss
{% raw %}

$$L=-\log\bigg(\frac{e^{s_y}}{\sum_me^{s_m}}\bigg)$${% endraw %}
除了loss外，其他设定与<a href="#mcsvml">Multiclass SVM loss</a>一样，求解梯度一样需要以下四个步骤：
1. 求{% raw %}$\partial L/\partial s_j${% endraw %}
2. 求{% raw %}$\partial s_j/\partial W${% endraw %}
3. 带入链式法则
4. 求平均损失对W的梯度dW

因为只有loss不同，其他都是一样的，因此只有第一步求{% raw %}$\partial L/\partial s_j${% endraw %}不同。

### 求{% raw %}$\partial L/\partial s_j${% endraw %}(方法1)
{% raw %}

$$L=-\log\bigg(\frac{e^{s_y}}{\sum_me^{s_m}}\bigg)=\log\sum_me^{s_m}-s_y$${% endraw %}

{% raw %}

$$\begin{aligned}\frac{\partial L}{\partial s_j}
&= \frac{1}{\sum_me^{s_m}}\frac{\partial (\sum_me^{s_m})}{\partial s_j}-\frac{\partial s_y}{\partial s_j}\\
&= \frac{1}{\sum_me^{s_m}}\cdot \sum_m\frac{\partial e^{s_m}}{\partial s_j}-\frac{\partial s_y}{\partial s_j} \\
&= \frac{1}{\sum_me^{s_m}}\cdot \sum_m 1\{m=j\}e^{s_m}-1\{j=y\} \\
&= \frac{e^{s_j}}{\sum_me^{s_m}}-1\{y=j\} \\
&= p_j - 1\{j=y\}
\end{aligned}$${% endraw %}
其中{% raw %}

$$p_j = \frac{e^{s_j}}{\sum_m e^{s_m}}$${% endraw %}


### 求{% raw %}$\partial L/\partial s_j${% endraw %}(方法2)
之所以要再多写个方法二，是为了增加对这种逻辑求导运算的熟练度。
{% raw %}

$$L=-\log\bigg(\frac{e^{s_y}}{\sum_me^{s_m}}\bigg)=-\log(p_y)$${% endraw %}
则
{% raw %}

$$\frac{\partial L}{\partial s_j}=-\frac{1}{p_y} \cdot \frac{\partial p_y}{\partial s_j}$${% endraw %}
其中
{% raw %}

$$\begin{aligned}\frac{\partial p_y}{\partial s_j}
&= \frac{\partial}{\partial s_j} \bigg( \frac{e^{s_y}}{\sum_m e^{s_m}}\bigg)\\
&= 1\{j=y\}\frac{e^{s_y}(\sum_m e^{s_m})-e^{s_y}e^{s_j}}{\big(\sum_m e^{s_m}\big)^2} - 1\{j \neq y\}\frac{e^{s_y}e^{s_j}}{\big(\sum_m e^{s_m}\big)^2} 
\end{aligned}$${% endraw %}

将其带入{% raw %}$\partial L/\partial s_j${% endraw %}得，
{% raw %}

$$\begin{aligned}\frac{\partial L}{\partial s_j}
&= -\frac{\sum_me^{s_m}}{e^{s_y}}\bigg(1\{j=y\}\frac{e^{s_y}(\sum_m e^{s_m})-e^{s_y}e^{s_j}}{\big(\sum_m e^{s_m}\big)^2} - 1\{j \neq y\}\frac{e^{s_y}e^{s_j}}{\big(\sum_m e^{s_m}\big)^2}\bigg) \\
&= -\bigg(1\{j=y\}\frac{(\sum_m e^{s_m})-e^{s_j}}{\sum_m e^{s_m}} - 1\{j \neq y\}\frac{e^{s_j}}{\sum_m e^{s_m}}\bigg) \\
&= 1\{j \neq y\}\frac{e^{s_j}}{\sum_m e^{s_m}} -1\{j=y\}\frac{(\sum_m e^{s_m})-e^{s_j}}{\sum_m e^{s_m}} \\
&= 1\{j \neq y\}p_j-1\{j=y\}(1-p_j) \\
&= 1\{j \neq y\}p_j-1\{j=y\} + 1\{j=y\}p_j \\
&= p_j - 1\{j=y\}
\end{aligned}$${% endraw %}

> **注意：** <br>
> 上式最后一步用了一个很简单的或运算
>
> {% raw %}$$1\{j \neq y\}p_j + 1\{j=y\}p_j = 1\{j \neq y \lor j=y\}p_j=p_j$${% endraw %}

### 求dW
{% raw %}

$$\begin{aligned}dW
&= \frac{1}{N}X^T \cdot dS
\end{aligned}$${% endraw %}
其中矩阵dS中的元素为
{% raw %}

$$\begin{aligned}(dS)_{ij} &= \frac{\partial L^{(i)}}{\partial s_j^{(i)}} \\
&= p_j - 1\{j=y^{(i)}\}
\end{aligned}$${% endraw %}

### 编码实现

```python
def softmax_loss(W, X, y, reg):
  """
  Inputs:
    - W: A numpy array of shape (D, K) containing weights.
    - X: A numpy array of shape (N, D) containing a minibatch of data.
    - y: A numpy array of shape (N,) containing training labels; 
         y[i] = k means that X[i] has label k, where 0 <= k < K.
  """
  N = X.shape[0]
  s = X.dot(W)

  # 避免数值问题
  s_max = s.max(axis=1).reshape((N, 1))
  exp_s = np.exp(s - s_max) 

  # 求dS
  p = exp_s / exp_s.sum(axis=1, keepdims=True)
  j_eq_y = np.zeros_like(p)
  j_eq_y[i, y] = 1.0
  dS = p - j_eq_y

  # 求dW
  dW = X.T.dot(dS) / N
  dW += reg * 2 * W # 正则项的梯度
```

# 总结
通过上面两个例子，求一层全连接神经网络的梯度计算基本理清楚了，不带正则项部分的计算被限制在这种形式下
{% raw %}

$$\begin{aligned}dW
&= \frac{1}{N}X^T \cdot dS
\end{aligned}$${% endraw %}
其中矩阵dS中元素需要根据loss推导，为了便于推导，本小学生总结了一些逻辑运算的技巧，用起来感觉方便了许多。